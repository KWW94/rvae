{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, expierment_name\n",
    "from model import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime('%Y-%b-%d-%H-%M-%S', time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.join('won', ts)\n",
    "os.makedirs('.\\\\'+save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'valid'] + (['test'] if False else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'valid']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    datasets[split] = PTB(\n",
    "        data_dir='data',\n",
    "        split=split,\n",
    "        create_data=False,\n",
    "        max_sequence_length=60,\n",
    "        min_occ=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = datasets['train'].vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_idx=datasets['train'].sos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_idx=datasets['train'].eos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx=datasets['train'].pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 60\n",
    "embedding_size = 300\n",
    "hidden_size = 256\n",
    "word_dropout = 0.5\n",
    "latent_size = 16\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "batch_size_fit = 32\n",
    "rnn_type = 'rnn'\n",
    "learning_rate = 0.001\n",
    "k = 0.0025\n",
    "x0 = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, max_sequence_length, hidden_size, word_dropout, latent_size,\n",
    "                sos_idx, eos_idx, pad_idx, rnn_type='rnn' , num_layers=1, bidirectional=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        #self.outputs2vocab = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n",
    "        self.encoder = Encoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional = bidirectional,latent_size = latent_size,rnn_type = rnn_type).cuda()\n",
    "        self.decoder = Decoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional = bidirectional,latent_size = latent_size,rnn_type = rnn_type).cuda()\n",
    "    \n",
    "    def forward(self,x,length):\n",
    "        \n",
    "        mu,logvar,reparam = self.encoder(Variable(x),length)\n",
    "        logp  = self.decoder(Variable(x),reparam)\n",
    "        \n",
    "        \n",
    "        return logp, mu, logvar, reparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, latent_size, bidirectional=True, num_layers = 1,rnn_type='rnn'):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.latent_size = latent_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.rnn_type == 'rnn':\n",
    "            rnn = nn.RNN\n",
    "        elif self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        elif self.rnn_type =='lstm':\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.encoder = rnn(self.embedding_size, self.hidden_size, num_layers = self.num_layers, bidirectional = self.bidirectional, batch_first = True)\n",
    "        \n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        self.hidden2mean = nn.Linear(self.hidden_size* self.hidden_factor, self.latent_size)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size* self.hidden_factor, self.latent_size)\n",
    "                        \n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        \n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps).cuda()\n",
    "        \n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def forward(self,x,length):\n",
    "        batch_size = x.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n",
    "        input_sequence = x[sorted_idx.cuda()]\n",
    "        #print(input_sequence)\n",
    "        _,hidden = self.encoder(self.embedding(input_sequence))\n",
    "        if self.bidirectional or self.num_layers > 1:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size*self.hidden_factor)\n",
    "        else:\n",
    "            hidden = hidden.squeeze()\n",
    "\n",
    "        mu = self.hidden2mean(hidden)\n",
    "        logvar = self.hidden2logv(hidden)\n",
    "\n",
    "        reparam = self.reparametrize(mu,logvar)\n",
    "        \n",
    "        return mu,logvar,reparam\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, latent_size, bidirectional=True, num_layers = 1,rnn_type='rnn',word_dropout = 0.5):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.latent_size = latent_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.rnn_type == 'rnn':\n",
    "            rnn = nn.RNN\n",
    "        elif self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        elif self.rnn_type =='lstm':\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers            \n",
    "        self.latent2hidden = nn.Linear(latent_size, hidden_size * self.hidden_factor)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.word_dropout = nn.Dropout(p=word_dropout)\n",
    "        \n",
    "        self.decoder = rnn(embedding_size, hidden_size, num_layers=num_layers, bidirectional=self.bidirectional, batch_first=True)\n",
    "        self.outputs2vocab = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n",
    "        \n",
    "    def forward(self,x,z):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n",
    "        input_sequence = x[sorted_idx.cuda()]\n",
    "        \n",
    "        hidden = self.latent2hidden(z)\n",
    "        if self.bidirectional or self.num_layers > 1:\n",
    "            # unflatten hidden state\n",
    "            hidden = hidden.view(self.hidden_factor, batch_size, self.hidden_size)\n",
    "        else:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            \n",
    "        outputs,_ = self.decoder(self.embedding(input_sequence),hidden)\n",
    "        \n",
    "        logp =nn.functional.log_softmax(self.outputs2vocab(outputs))\n",
    "        \n",
    "        return logp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvae=RVAE(vocab_size, embedding_size, max_sequence_length, hidden_size, word_dropout, latent_size,sos_idx, eos_idx, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1/(1+np.exp(-k*(step-x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1, step/x0)\n",
    "\n",
    "NLL = torch.nn.NLLLoss(size_average=False, ignore_index=datasets['train'].pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0):\n",
    "\n",
    "    # cut-off unnecessary padding from target, and flatten\n",
    "    target = target[:, :60].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(2))\n",
    "\n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = NLL(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rvae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RVAE(\n",
      "  (outputs2vocab): Linear(in_features=512, out_features=9877, bias=True)\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(9877, 300)\n",
      "    (encoder): RNN(300, 256, batch_first=True, bidirectional=True)\n",
      "    (hidden2mean): Linear(in_features=512, out_features=16, bias=True)\n",
      "    (hidden2logv): Linear(in_features=512, out_features=16, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (latent2hidden): Linear(in_features=16, out_features=512, bias=True)\n",
      "    (embedding): Embedding(9877, 300)\n",
      "    (word_dropout): Dropout(p=0.5)\n",
      "    (decoder): RNN(300, 256, batch_first=True, bidirectional=True)\n",
      "    (outputs2vocab): Linear(in_features=512, out_features=9877, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asd13\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/1314, Loss   87.2974, NLL-Loss   87.2956, KL-Loss    0.9128, KL-Weight  0.002\n",
      "TRAIN Batch 0100/1314, Loss   73.6464, NLL-Loss   73.6410, KL-Loss    2.1666, KL-Weight  0.002\n",
      "TRAIN Batch 0200/1314, Loss   85.0424, NLL-Loss   85.0384, KL-Loss    1.2654, KL-Weight  0.003\n",
      "TRAIN Batch 0300/1314, Loss   86.8569, NLL-Loss   86.8531, KL-Loss    0.9415, KL-Weight  0.004\n",
      "TRAIN Batch 0400/1314, Loss   81.6852, NLL-Loss   81.6788, KL-Loss    1.2377, KL-Weight  0.005\n",
      "TRAIN Batch 0500/1314, Loss   73.4471, NLL-Loss   73.4420, KL-Loss    0.7675, KL-Weight  0.007\n",
      "TRAIN Batch 0600/1314, Loss   81.3431, NLL-Loss   81.3404, KL-Loss    0.3150, KL-Weight  0.009\n",
      "TRAIN Batch 0700/1314, Loss   75.1611, NLL-Loss   75.1556, KL-Loss    0.4995, KL-Weight  0.011\n",
      "TRAIN Batch 0800/1314, Loss   75.6880, NLL-Loss   75.6834, KL-Loss    0.3261, KL-Weight  0.014\n",
      "TRAIN Batch 0900/1314, Loss   94.4389, NLL-Loss   94.4341, KL-Loss    0.2635, KL-Weight  0.018\n",
      "TRAIN Batch 1000/1314, Loss   79.2838, NLL-Loss   79.2805, KL-Loss    0.1455, KL-Weight  0.023\n",
      "TRAIN Batch 1100/1314, Loss   69.9575, NLL-Loss   69.9551, KL-Loss    0.0814, KL-Weight  0.029\n",
      "TRAIN Batch 1200/1314, Loss   73.7491, NLL-Loss   73.7461, KL-Loss    0.0809, KL-Weight  0.037\n",
      "TRAIN Batch 1300/1314, Loss   83.4063, NLL-Loss   83.4033, KL-Loss    0.0635, KL-Weight  0.047\n",
      "TRAIN Batch 1314/1314, Loss   70.2539, NLL-Loss   70.2514, KL-Loss    0.0503, KL-Weight  0.049\n",
      "Model saved at won\\2018-May-10-08-38-57\\E0.pytorch\n",
      "TRAIN Epoch 00/10, Mean ELBO   79.1297\n",
      "VALID Batch 0000/105, Loss   81.0026, NLL-Loss   81.0010, KL-Loss    0.0326, KL-Weight  0.049\n",
      "VALID Batch 0100/105, Loss   61.6244, NLL-Loss   61.6228, KL-Loss    0.0325, KL-Weight  0.049\n",
      "VALID Batch 0105/105, Loss   52.2630, NLL-Loss   52.2614, KL-Loss    0.0326, KL-Weight  0.049\n",
      "VALID Epoch 00/10, Mean ELBO   79.0555\n",
      "TRAIN Batch 0000/1314, Loss   73.4382, NLL-Loss   73.4366, KL-Loss    0.0326, KL-Weight  0.049\n",
      "TRAIN Batch 0100/1314, Loss   80.0537, NLL-Loss   80.0472, KL-Loss    0.1038, KL-Weight  0.062\n",
      "TRAIN Batch 0200/1314, Loss  102.4792, NLL-Loss  102.4749, KL-Loss    0.0549, KL-Weight  0.079\n",
      "TRAIN Batch 0300/1314, Loss   74.8506, NLL-Loss   74.8474, KL-Loss    0.0323, KL-Weight  0.099\n",
      "TRAIN Batch 0400/1314, Loss   89.8943, NLL-Loss   89.8906, KL-Loss    0.0297, KL-Weight  0.123\n",
      "TRAIN Batch 0500/1314, Loss   76.7796, NLL-Loss   76.7782, KL-Loss    0.0092, KL-Weight  0.153\n",
      "TRAIN Batch 0600/1314, Loss   90.7807, NLL-Loss   90.7785, KL-Loss    0.0115, KL-Weight  0.188\n",
      "TRAIN Batch 0700/1314, Loss   66.0682, NLL-Loss   66.0661, KL-Loss    0.0093, KL-Weight  0.229\n",
      "TRAIN Batch 0800/1314, Loss   75.7243, NLL-Loss   75.7194, KL-Loss    0.0178, KL-Weight  0.276\n",
      "TRAIN Batch 0900/1314, Loss   75.9246, NLL-Loss   75.9198, KL-Loss    0.0145, KL-Weight  0.329\n",
      "TRAIN Batch 1000/1314, Loss   71.9315, NLL-Loss   71.9291, KL-Loss    0.0062, KL-Weight  0.386\n",
      "TRAIN Batch 1100/1314, Loss   90.1189, NLL-Loss   90.1162, KL-Loss    0.0060, KL-Weight  0.447\n",
      "TRAIN Batch 1200/1314, Loss   73.1229, NLL-Loss   73.1183, KL-Loss    0.0090, KL-Weight  0.509\n",
      "TRAIN Batch 1300/1314, Loss   84.3658, NLL-Loss   84.3627, KL-Loss    0.0054, KL-Weight  0.571\n",
      "TRAIN Batch 1314/1314, Loss   72.6506, NLL-Loss   72.6479, KL-Loss    0.0046, KL-Weight  0.580\n",
      "Model saved at won\\2018-May-10-08-38-57\\E1.pytorch\n",
      "TRAIN Epoch 01/10, Mean ELBO   79.0920\n",
      "VALID Batch 0000/105, Loss   81.0053, NLL-Loss   81.0030, KL-Loss    0.0038, KL-Weight  0.581\n",
      "VALID Batch 0100/105, Loss   61.6234, NLL-Loss   61.6212, KL-Loss    0.0038, KL-Weight  0.581\n",
      "VALID Batch 0105/105, Loss   52.2866, NLL-Loss   52.2843, KL-Loss    0.0038, KL-Weight  0.581\n",
      "VALID Epoch 01/10, Mean ELBO   79.0563\n",
      "TRAIN Batch 0000/1314, Loss   89.6655, NLL-Loss   89.6633, KL-Loss    0.0038, KL-Weight  0.581\n",
      "TRAIN Batch 0100/1314, Loss   82.1052, NLL-Loss   82.1025, KL-Loss    0.0042, KL-Weight  0.640\n",
      "TRAIN Batch 0200/1314, Loss   86.7666, NLL-Loss   86.7545, KL-Loss    0.0174, KL-Weight  0.695\n",
      "TRAIN Batch 0300/1314, Loss   72.9123, NLL-Loss   72.9111, KL-Loss    0.0017, KL-Weight  0.746\n",
      "TRAIN Batch 0400/1314, Loss   87.7406, NLL-Loss   87.7381, KL-Loss    0.0032, KL-Weight  0.790\n",
      "TRAIN Batch 0500/1314, Loss   81.0434, NLL-Loss   81.0417, KL-Loss    0.0021, KL-Weight  0.828\n",
      "TRAIN Batch 0600/1314, Loss   78.5918, NLL-Loss   78.5800, KL-Loss    0.0137, KL-Weight  0.861\n",
      "TRAIN Batch 0700/1314, Loss   70.9425, NLL-Loss   70.9386, KL-Loss    0.0043, KL-Weight  0.888\n",
      "TRAIN Batch 0800/1314, Loss   71.9202, NLL-Loss   71.9186, KL-Loss    0.0017, KL-Weight  0.911\n",
      "TRAIN Batch 0900/1314, Loss   86.3176, NLL-Loss   86.3171, KL-Loss    0.0005, KL-Weight  0.929\n",
      "TRAIN Batch 1000/1314, Loss   70.5173, NLL-Loss   70.5163, KL-Loss    0.0010, KL-Weight  0.944\n",
      "TRAIN Batch 1100/1314, Loss   76.1242, NLL-Loss   76.1198, KL-Loss    0.0046, KL-Weight  0.956\n",
      "TRAIN Batch 1200/1314, Loss   80.8203, NLL-Loss   80.8186, KL-Loss    0.0017, KL-Weight  0.965\n",
      "TRAIN Batch 1300/1314, Loss   78.9136, NLL-Loss   78.9128, KL-Loss    0.0008, KL-Weight  0.973\n",
      "TRAIN Batch 1314/1314, Loss   72.6959, NLL-Loss   72.6954, KL-Loss    0.0005, KL-Weight  0.974\n",
      "Model saved at won\\2018-May-10-08-38-57\\E2.pytorch\n",
      "TRAIN Epoch 02/10, Mean ELBO   79.0907\n",
      "VALID Batch 0000/105, Loss   81.0119, NLL-Loss   81.0107, KL-Loss    0.0012, KL-Weight  0.974\n",
      "VALID Batch 0100/105, Loss   61.6074, NLL-Loss   61.6062, KL-Loss    0.0012, KL-Weight  0.974\n",
      "VALID Batch 0105/105, Loss   52.3651, NLL-Loss   52.3639, KL-Loss    0.0012, KL-Weight  0.974\n",
      "VALID Epoch 02/10, Mean ELBO   79.0686\n",
      "TRAIN Batch 0000/1314, Loss   84.2836, NLL-Loss   84.2824, KL-Loss    0.0012, KL-Weight  0.974\n",
      "TRAIN Batch 0100/1314, Loss   86.5453, NLL-Loss   86.5417, KL-Loss    0.0036, KL-Weight  0.979\n",
      "TRAIN Batch 0200/1314, Loss   69.7145, NLL-Loss   69.6348, KL-Loss    0.0810, KL-Weight  0.984\n",
      "TRAIN Batch 0300/1314, Loss   75.9263, NLL-Loss   75.9261, KL-Loss    0.0002, KL-Weight  0.987\n",
      "TRAIN Batch 0400/1314, Loss   85.3342, NLL-Loss   85.3322, KL-Loss    0.0020, KL-Weight  0.990\n",
      "TRAIN Batch 0500/1314, Loss   67.0334, NLL-Loss   67.0333, KL-Loss    0.0002, KL-Weight  0.992\n",
      "TRAIN Batch 0600/1314, Loss   69.6353, NLL-Loss   69.6348, KL-Loss    0.0005, KL-Weight  0.994\n",
      "TRAIN Batch 0700/1314, Loss   69.0986, NLL-Loss   69.0981, KL-Loss    0.0004, KL-Weight  0.995\n",
      "TRAIN Batch 0800/1314, Loss   81.5534, NLL-Loss   81.5530, KL-Loss    0.0004, KL-Weight  0.996\n",
      "TRAIN Batch 0900/1314, Loss   73.1239, NLL-Loss   73.1238, KL-Loss    0.0001, KL-Weight  0.997\n",
      "TRAIN Batch 1000/1314, Loss   74.9466, NLL-Loss   74.9461, KL-Loss    0.0004, KL-Weight  0.998\n",
      "TRAIN Batch 1100/1314, Loss   87.8386, NLL-Loss   87.8379, KL-Loss    0.0006, KL-Weight  0.998\n",
      "TRAIN Batch 1200/1314, Loss   83.6032, NLL-Loss   83.5977, KL-Loss    0.0056, KL-Weight  0.999\n",
      "TRAIN Batch 1300/1314, Loss   77.3264, NLL-Loss   77.3258, KL-Loss    0.0006, KL-Weight  0.999\n",
      "TRAIN Batch 1314/1314, Loss   75.9504, NLL-Loss   75.9478, KL-Loss    0.0026, KL-Weight  0.999\n",
      "Model saved at won\\2018-May-10-08-38-57\\E3.pytorch\n",
      "TRAIN Epoch 03/10, Mean ELBO   79.0857\n",
      "VALID Batch 0000/105, Loss   81.0136, NLL-Loss   81.0114, KL-Loss    0.0022, KL-Weight  0.999\n",
      "VALID Batch 0100/105, Loss   61.6420, NLL-Loss   61.6399, KL-Loss    0.0022, KL-Weight  0.999\n",
      "VALID Batch 0105/105, Loss   52.2580, NLL-Loss   52.2558, KL-Loss    0.0022, KL-Weight  0.999\n",
      "VALID Epoch 03/10, Mean ELBO   79.0688\n",
      "TRAIN Batch 0000/1314, Loss   75.7091, NLL-Loss   75.7069, KL-Loss    0.0022, KL-Weight  0.999\n",
      "TRAIN Batch 0100/1314, Loss   68.6673, NLL-Loss   68.6651, KL-Loss    0.0023, KL-Weight  0.999\n",
      "TRAIN Batch 0200/1314, Loss   72.1253, NLL-Loss   72.1245, KL-Loss    0.0008, KL-Weight  0.999\n",
      "TRAIN Batch 0300/1314, Loss   69.8589, NLL-Loss   69.8545, KL-Loss    0.0045, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1314, Loss   68.4600, NLL-Loss   68.4586, KL-Loss    0.0014, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1314, Loss   80.0225, NLL-Loss   80.0207, KL-Loss    0.0017, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1314, Loss   82.3163, NLL-Loss   82.3157, KL-Loss    0.0006, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1314, Loss   71.3708, NLL-Loss   71.3677, KL-Loss    0.0031, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1314, Loss   78.5254, NLL-Loss   78.5186, KL-Loss    0.0068, KL-Weight  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0900/1314, Loss   75.1706, NLL-Loss   75.1696, KL-Loss    0.0010, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1314, Loss   79.1760, NLL-Loss   79.1729, KL-Loss    0.0031, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1314, Loss   84.6929, NLL-Loss   84.6924, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1314, Loss   82.2200, NLL-Loss   82.2193, KL-Loss    0.0007, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1314, Loss   82.1853, NLL-Loss   82.1842, KL-Loss    0.0012, KL-Weight  1.000\n",
      "TRAIN Batch 1314/1314, Loss   65.6020, NLL-Loss   65.6011, KL-Loss    0.0009, KL-Weight  1.000\n",
      "Model saved at won\\2018-May-10-08-38-57\\E4.pytorch\n",
      "TRAIN Epoch 04/10, Mean ELBO   79.0946\n",
      "VALID Batch 0000/105, Loss   81.0051, NLL-Loss   81.0043, KL-Loss    0.0008, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss   61.6299, NLL-Loss   61.6291, KL-Loss    0.0008, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss   52.2723, NLL-Loss   52.2716, KL-Loss    0.0008, KL-Weight  1.000\n",
      "VALID Epoch 04/10, Mean ELBO   79.0803\n",
      "TRAIN Batch 0000/1314, Loss   85.5696, NLL-Loss   85.5688, KL-Loss    0.0008, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1314, Loss   75.0484, NLL-Loss   75.0484, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1314, Loss   63.9977, NLL-Loss   63.9977, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1314, Loss   77.2246, NLL-Loss   77.2193, KL-Loss    0.0053, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1314, Loss   73.5522, NLL-Loss   73.5517, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1314, Loss   83.9430, NLL-Loss   83.9429, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1314, Loss   81.3258, NLL-Loss   81.3254, KL-Loss    0.0004, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1314, Loss   78.5186, NLL-Loss   78.5181, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1314, Loss   80.4721, NLL-Loss   80.4672, KL-Loss    0.0049, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1314, Loss   66.6122, NLL-Loss   66.6117, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1314, Loss   83.4978, NLL-Loss   83.4977, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1314, Loss   87.8273, NLL-Loss   87.8267, KL-Loss    0.0006, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1314, Loss   77.9743, NLL-Loss   77.9736, KL-Loss    0.0007, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1314, Loss   77.4498, NLL-Loss   77.4433, KL-Loss    0.0066, KL-Weight  1.000\n",
      "TRAIN Batch 1314/1314, Loss   83.2822, NLL-Loss   83.2811, KL-Loss    0.0011, KL-Weight  1.000\n",
      "Model saved at won\\2018-May-10-08-38-57\\E5.pytorch\n",
      "TRAIN Epoch 05/10, Mean ELBO   79.0913\n",
      "VALID Batch 0000/105, Loss   81.0073, NLL-Loss   81.0062, KL-Loss    0.0012, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss   61.6397, NLL-Loss   61.6385, KL-Loss    0.0012, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss   52.2703, NLL-Loss   52.2691, KL-Loss    0.0012, KL-Weight  1.000\n",
      "VALID Epoch 05/10, Mean ELBO   79.0794\n",
      "TRAIN Batch 0000/1314, Loss   89.2432, NLL-Loss   89.2421, KL-Loss    0.0012, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1314, Loss   92.0562, NLL-Loss   92.0552, KL-Loss    0.0010, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1314, Loss   79.2724, NLL-Loss   79.2688, KL-Loss    0.0037, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1314, Loss   73.9777, NLL-Loss   73.9765, KL-Loss    0.0011, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1314, Loss   82.5339, NLL-Loss   82.5262, KL-Loss    0.0077, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1314, Loss   77.1100, NLL-Loss   77.1079, KL-Loss    0.0021, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1314, Loss   86.1066, NLL-Loss   86.1062, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1314, Loss   81.3399, NLL-Loss   81.3390, KL-Loss    0.0009, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1314, Loss   88.2692, NLL-Loss   88.2686, KL-Loss    0.0006, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1314, Loss   84.1613, NLL-Loss   84.1601, KL-Loss    0.0012, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1314, Loss   82.1037, NLL-Loss   82.1000, KL-Loss    0.0037, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1314, Loss   79.6049, NLL-Loss   79.6047, KL-Loss    0.0003, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1314, Loss   76.6342, NLL-Loss   76.6309, KL-Loss    0.0033, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1314, Loss   83.5103, NLL-Loss   83.5078, KL-Loss    0.0025, KL-Weight  1.000\n",
      "TRAIN Batch 1314/1314, Loss   73.2424, NLL-Loss   73.2421, KL-Loss    0.0003, KL-Weight  1.000\n",
      "Model saved at won\\2018-May-10-08-38-57\\E6.pytorch\n",
      "TRAIN Epoch 06/10, Mean ELBO   79.0876\n",
      "VALID Batch 0000/105, Loss   81.0199, NLL-Loss   81.0197, KL-Loss    0.0002, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss   61.6414, NLL-Loss   61.6413, KL-Loss    0.0002, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss   52.2652, NLL-Loss   52.2650, KL-Loss    0.0002, KL-Weight  1.000\n",
      "VALID Epoch 06/10, Mean ELBO   79.0793\n",
      "TRAIN Batch 0000/1314, Loss   91.1880, NLL-Loss   91.1878, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1314, Loss   81.3295, NLL-Loss   81.3292, KL-Loss    0.0003, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1314, Loss   81.9018, NLL-Loss   81.9014, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1314, Loss   78.6243, NLL-Loss   78.6240, KL-Loss    0.0003, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1314, Loss   77.2237, NLL-Loss   77.2211, KL-Loss    0.0026, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1314, Loss   90.7897, NLL-Loss   90.7782, KL-Loss    0.0114, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1314, Loss   78.4028, NLL-Loss   78.4025, KL-Loss    0.0003, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1314, Loss   81.0033, NLL-Loss   81.0026, KL-Loss    0.0007, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1314, Loss   81.3475, NLL-Loss   81.3464, KL-Loss    0.0011, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1314, Loss   83.4407, NLL-Loss   83.4406, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1314, Loss   73.7989, NLL-Loss   73.7987, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1314, Loss   87.4183, NLL-Loss   87.4163, KL-Loss    0.0020, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1314, Loss   82.6274, NLL-Loss   82.6266, KL-Loss    0.0008, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1314, Loss   81.5491, NLL-Loss   81.5426, KL-Loss    0.0066, KL-Weight  1.000\n",
      "TRAIN Batch 1314/1314, Loss   78.3538, NLL-Loss   78.3512, KL-Loss    0.0026, KL-Weight  1.000\n",
      "Model saved at won\\2018-May-10-08-38-57\\E7.pytorch\n",
      "TRAIN Epoch 07/10, Mean ELBO   79.1020\n",
      "VALID Batch 0000/105, Loss   81.0119, NLL-Loss   81.0094, KL-Loss    0.0025, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss   61.6339, NLL-Loss   61.6314, KL-Loss    0.0025, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss   52.2818, NLL-Loss   52.2793, KL-Loss    0.0025, KL-Weight  1.000\n",
      "VALID Epoch 07/10, Mean ELBO   79.0933\n",
      "TRAIN Batch 0000/1314, Loss   81.9850, NLL-Loss   81.9825, KL-Loss    0.0025, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1314, Loss   80.6035, NLL-Loss   80.6026, KL-Loss    0.0010, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1314, Loss   81.6611, NLL-Loss   81.6607, KL-Loss    0.0004, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1314, Loss   90.7702, NLL-Loss   90.7686, KL-Loss    0.0016, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1314, Loss   86.9618, NLL-Loss   86.9614, KL-Loss    0.0004, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1314, Loss   74.9414, NLL-Loss   74.9398, KL-Loss    0.0016, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1314, Loss   84.8036, NLL-Loss   84.8033, KL-Loss    0.0003, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1314, Loss   77.6574, NLL-Loss   77.6572, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1314, Loss   79.2712, NLL-Loss   79.2707, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1314, Loss   87.0778, NLL-Loss   87.0768, KL-Loss    0.0010, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1314, Loss   78.5413, NLL-Loss   78.5321, KL-Loss    0.0092, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1314, Loss   82.7382, NLL-Loss   82.7381, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1314, Loss   82.2970, NLL-Loss   82.2967, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1314, Loss   91.6468, NLL-Loss   91.6462, KL-Loss    0.0006, KL-Weight  1.000\n",
      "TRAIN Batch 1314/1314, Loss   75.0029, NLL-Loss   74.9918, KL-Loss    0.0111, KL-Weight  1.000\n",
      "Model saved at won\\2018-May-10-08-38-57\\E8.pytorch\n",
      "TRAIN Epoch 08/10, Mean ELBO   79.1083\n",
      "VALID Batch 0000/105, Loss   80.9915, NLL-Loss   80.9797, KL-Loss    0.0117, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss   61.6909, NLL-Loss   61.6791, KL-Loss    0.0117, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss   52.3483, NLL-Loss   52.3366, KL-Loss    0.0117, KL-Weight  1.000\n",
      "VALID Epoch 08/10, Mean ELBO   79.1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/1314, Loss   74.5312, NLL-Loss   74.5195, KL-Loss    0.0117, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1314, Loss   75.5702, NLL-Loss   75.5700, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1314, Loss   80.4788, NLL-Loss   80.4704, KL-Loss    0.0083, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1314, Loss   78.6548, NLL-Loss   78.6541, KL-Loss    0.0007, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1314, Loss   77.2497, NLL-Loss   77.2496, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1314, Loss   74.8573, NLL-Loss   74.8557, KL-Loss    0.0015, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1314, Loss   79.1657, NLL-Loss   79.1654, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1314, Loss   89.2990, NLL-Loss   89.2989, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1314, Loss   77.2643, NLL-Loss   77.2638, KL-Loss    0.0005, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1314, Loss   74.4014, NLL-Loss   74.3337, KL-Loss    0.0676, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1314, Loss   83.2737, NLL-Loss   83.2735, KL-Loss    0.0002, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1314, Loss   77.3452, NLL-Loss   74.8058, KL-Loss    2.5394, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1314, Loss   75.8909, NLL-Loss   75.8908, KL-Loss    0.0001, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1314, Loss   77.9961, NLL-Loss   77.9961, KL-Loss    0.0000, KL-Weight  1.000\n",
      "TRAIN Batch 1314/1314, Loss   69.6883, NLL-Loss   69.6880, KL-Loss    0.0003, KL-Weight  1.000\n",
      "Model saved at won\\2018-May-10-08-38-57\\E9.pytorch\n",
      "TRAIN Epoch 09/10, Mean ELBO   79.1197\n",
      "VALID Batch 0000/105, Loss   80.9942, NLL-Loss   80.9939, KL-Loss    0.0003, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss   61.6467, NLL-Loss   61.6464, KL-Loss    0.0003, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss   52.2653, NLL-Loss   52.2650, KL-Loss    0.0003, KL-Weight  1.000\n",
      "VALID Epoch 09/10, Mean ELBO   79.1125\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "avg_losses = []\n",
    "losses = []\n",
    "NLL_losses = []\n",
    "KL_losses = []\n",
    "KL_weights = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for split in splits:\n",
    "        data_loader = DataLoader(\n",
    "                    dataset=datasets[split],\n",
    "                    batch_size=batch_size_fit,\n",
    "                    shuffle=split=='train',\n",
    "                    num_workers=cpu_count(),\n",
    "                    pin_memory=torch.cuda.is_available()\n",
    "                )\n",
    "\n",
    "        if split == 'train':\n",
    "            rvae.train()\n",
    "        else:\n",
    "            rvae.eval()\n",
    "            \n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            batch_size = batch['input'].size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = batch['input'].type(torch.cuda.LongTensor)\n",
    "            length = batch['length']\n",
    "            \n",
    "            logp, mean, logv, z=rvae(x,length)\n",
    "            \n",
    "            NLL_loss, KL_loss, KL_weight = loss_fn(logp, Variable(batch['target']).type(torch.cuda.LongTensor),batch['length'], mean, logv, 'logistic', step, k, x0)\n",
    "            \n",
    "            loss = (NLL_loss + KL_weight * KL_loss)/batch_size\n",
    "            \n",
    "            losses.append(float(loss.cpu().data))\n",
    "            NLL_losses.append(NLL_loss.data[0]/batch_size)\n",
    "            KL_losses.append(KL_loss.data[0]/batch_size)\n",
    "            KL_weights.append(KL_weight)\n",
    "            \n",
    "\n",
    "            \n",
    "            if split == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            if iteration % 100 == 0 or iteration+1 == len(data_loader):\n",
    "                print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "            %(split.upper(), iteration, len(data_loader)-1, loss.data[0], NLL_loss.data[0]/batch_size, KL_loss.data[0]/batch_size, KL_weight))\n",
    "                np.savez(L=losses,file='loss.npz')\n",
    "                np.savez(L=NLL_losses,file='NLL_losses.npz')\n",
    "                np.savez(L=KL_losses,file='KL_losses.npz') \n",
    "                np.savez(L=KL_weights,file='KL_weights.npz')\n",
    "                \n",
    "         # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(rvae.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, epochs, np.mean(np.array(losses))))\n",
    "        avg_losses.append(np.mean(np.array(losses)))\n",
    "        np.savez(L=avg_losses,file='avg_losses.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-840057ed109c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w2i'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'i2w'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs2vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import to_var, idx2word, interpolate\n",
    "\n",
    "with open('data'+'/ptb.vocab.json', 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "w2i, i2w = vocab['w2i'], vocab['i2w']\n",
    "\n",
    "samples = torch.topk(nn.functional.softmax(rvae.outputs2vocab(outputs)),1,dim=-1)[1].squeeze()\n",
    "\n",
    "samples = samples.cpu().data.numpy()\n",
    "\n",
    "sent_str = [str()]*len(samples)\n",
    "\n",
    "for i, sent in enumerate(samples):\n",
    "    for word_id in sent:\n",
    "        if word_id == w2i['<pad>']: \n",
    "            break\n",
    "        sent_str[i] += i2w[str(word_id)] + \" \"\n",
    "    sent_str[i] = sent_str[i].strip()\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
